{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import cpu_count  #统计本机cpu数量\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.fluid as fluid"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据预处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成字典结束.\n",
      "生成训练集/测试集结束.\n"
     ]
    }
   ],
   "source": [
    "#　定义一组公共变量\n",
    "data_root = \"data/\"  # 数据集所在目录\n",
    "data_file = \"news_classify_data.txt\"  # 原始数据集\n",
    "train_file = \"train.txt\"  # 训练集文件\n",
    "test_file = \"test.txt\"  # 测试集文件\n",
    "dict_file = \"dict_txt.txt\"  # 字典文件(存放字和编码映射关系)\n",
    "\n",
    "data_file_path = data_root + data_file  # 数据集完整路径\n",
    "train_file_path = data_root + train_file  # 训练集文件完整路径\n",
    "test_file_path = data_root + test_file  # 测试集文件完整路径\n",
    "dict_file_path = data_root + dict_file  # 字典文件完整路径\n",
    "\n",
    "\n",
    "# 取出样本中所有字，对每个字进行编码，将编码结果存入字典文件\n",
    "def create_dict():\n",
    "    dict_set = set()  # 集合，用作去重\n",
    "    with open(data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():  # 遍历每行\n",
    "            line = line.replace(\"\\n\", \"\")  # 去除换行符\n",
    "            tmp_list = line.split(\"_!_\")  # 根据分隔符拆分\n",
    "            title = tmp_list[-1]  # 最后一个字段即为标题\n",
    "            for word in title:  # 取出每个字\n",
    "                dict_set.add(word)\n",
    "\n",
    "    # 遍历集合，取出每个字进行编号\n",
    "    dict_txt = {}  # 定义字典\n",
    "    i = 1  # 编码使用的计数器\n",
    "    for word in dict_set:\n",
    "        dict_txt[word] = i  # 字-编码 键值对添加到字典\n",
    "        i += 1\n",
    "\n",
    "    dict_txt[\"<unk>\"] = i  # 未知字符(在样本中未出现过的字)\n",
    "\n",
    "    # 将字典内容存入文件\n",
    "    with open(dict_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(dict_txt))\n",
    "\n",
    "    print(\"生成字典结束.\")\n",
    "\n",
    "\n",
    "# 传入一个句子，将每个字替换为编码值，和标签一起返回\n",
    "def line_encoding(title, dict_txt, label):\n",
    "    new_line = \"\"  # 编码结果\n",
    "    for word in title:\n",
    "        if word in dict_txt:  # 在字典中\n",
    "            code = str(dict_txt[word])  # 取出编码值\n",
    "        else:  # 不在字典中\n",
    "            code = str(dict_txt[\"<unk>\"])  # 取未知字符编码值\n",
    "        new_line = new_line + code + \",\"  # 追加到字符串后面\n",
    "    new_line = new_line[:-1]  # 去掉最后一个多余的逗号\n",
    "    new_line = new_line + \"\\t\" + label + \"\\n\"  # 追加标签值\n",
    "    return new_line\n",
    "\n",
    "\n",
    "# 读取原始样本，取出标题部分进行编码，将编码后的划分测试集/训练集\n",
    "def create_train_test_file():\n",
    "    # 清空训练集/测试集\n",
    "    with open(train_file_path, \"w\") as f:\n",
    "        pass\n",
    "    with open(test_file_path, \"w\") as f:\n",
    "        pass\n",
    "\n",
    "    # 读取字典文件\n",
    "    with open(dict_file_path, \"r\", encoding=\"utf-8\") as f_dict:\n",
    "        dict_txt = eval(f_dict.readlines()[0])  # 读取字典文件第一行，生成字典对象\n",
    "\n",
    "    # 读取原始样本\n",
    "    with open(data_file_path, \"r\", encoding=\"utf-8\") as f_data:\n",
    "        lines = f_data.readlines()\n",
    "\n",
    "    i = 0\n",
    "    for line in lines:\n",
    "        tmp_list = line.replace(\"\\n\", \"\").split(\"_!_\")  # 拆分\n",
    "        title = tmp_list[3]  # 标题\n",
    "        label = tmp_list[1]  # 类别\n",
    "        new_line = line_encoding(title, dict_txt, label)  # 对标题编码\n",
    "\n",
    "        if i % 10 == 0:  # 写入测试集\n",
    "            with open(test_file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(new_line)\n",
    "        else:  # 写入训练集\n",
    "            with open(train_file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(new_line)\n",
    "        i += 1\n",
    "    print(\"生成训练集/测试集结束.\")\n",
    "\n",
    "\n",
    "create_dict()  # 根据样本生成字典\n",
    "create_train_test_file()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型定义与训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 读取字典文件，返回字典长度（生成词向量时使用）\n",
    "def get_dict_len(dict_path):\n",
    "    with open(dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        dict_txt = eval(f.readlines()[0])\n",
    "    return len(dict_txt.keys())\n",
    "\n",
    "\n",
    "def data_mapper(sample):\n",
    "    data, label = sample  # 赋值到变量\n",
    "    val = [int(w) for w in data.split(\",\")]  # 将编码值转换位数字(从文件读取为字符串)\n",
    "    return val, int(label)\n",
    "\n",
    "\n",
    "def train_reader(train_file_path):  # 训练集读取器\n",
    "    def reader():\n",
    "        with open(train_file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            np.random.shuffle(lines)  # 随机化处理\n",
    "            for line in lines:\n",
    "                data, label = line.split(\"\\t\")  # 拆分\n",
    "                yield data, label\n",
    "\n",
    "    return paddle.reader.xmap_readers(data_mapper, reader, cpu_count(), 1024)\n",
    "\n",
    "\n",
    "def test_reader(test_file_path):  # 训练集读取器\n",
    "    def reader():\n",
    "        with open(test_file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                data, label = line.split(\"\\t\")  # 拆分\n",
    "                yield data, label\n",
    "\n",
    "    return paddle.reader.xmap_readers(data_mapper, reader, cpu_count(), 1024)\n",
    "\n",
    "\n",
    "# 定义网络\n",
    "def Text_CNN(data, dict_dim, class_dim=10, emb_dim=128,\n",
    "             hid_dim=128, hid_dim2=128):\n",
    "    \"\"\"\n",
    "    定义TextCNN模型\n",
    "    :param data:　输入\n",
    "    :param dict_dim:　词典大小(词语总的数量)\n",
    "    :param class_dim:　分类的数量\n",
    "    :param emb_dim: 词嵌入长度\n",
    "    :param hid_dim:　第一个卷基层卷积核数量\n",
    "    :param hid_dim2:　第二个卷基层卷积核数量\n",
    "    :return:　模型预测结果\n",
    "    \"\"\"\n",
    "    # embedding层\n",
    "    emb = fluid.layers.embedding(input=data, size=[dict_dim, emb_dim])\n",
    "    # 并列两个卷积/池化层\n",
    "    conv1 = fluid.nets.sequence_conv_pool(input=emb,  # 输入(词嵌入层输出)\n",
    "                                          num_filters=hid_dim,  #　卷积核数量\n",
    "                                          filter_size=3,  #卷积核大小\n",
    "                                          act=\"tanh\",  #激活函数\n",
    "                                          pool_type=\"sqrt\")  #池化类型\n",
    "    conv2 = fluid.nets.sequence_conv_pool(input=emb,  # 输入(词嵌入层输出)\n",
    "                                          num_filters=hid_dim2,  #　卷积核数量\n",
    "                                          filter_size=4,  #卷积核大小\n",
    "                                          act=\"tanh\",  #激活函数\n",
    "                                          pool_type=\"sqrt\")  #池化类型\n",
    "    # fc\n",
    "    output = fluid.layers.fc(input=[conv1, conv2],  # 输入\n",
    "                             size=class_dim,  #输出值个数\n",
    "                             act=\"softmax\")  #激活函数\n",
    "    return output\n",
    "\n",
    "\n",
    "# 定义占位符张量\n",
    "words = fluid.layers.data(name=\"words\",\n",
    "                          shape=[1],\n",
    "                          dtype=\"int64\",\n",
    "                          lod_level=1)  # LOD张量用来表示变长数据\n",
    "label = fluid.layers.data(name=\"label\",\n",
    "                          shape=[1],\n",
    "                          dtype=\"int64\")\n",
    "dict_dim = get_dict_len(dict_file_path)  # 获取字典长度\n",
    "# 调用模型函数\n",
    "model = Text_CNN(words, dict_dim)\n",
    "# 损失函数\n",
    "cost = fluid.layers.cross_entropy(input=model, label=label)\n",
    "avg_cost = fluid.layers.mean(cost)\n",
    "# 优化器\n",
    "optimizer = fluid.optimizer.Adam(learning_rate=0.0001)\n",
    "optimizer.minimize(avg_cost)\n",
    "# 准确率\n",
    "accuracy = fluid.layers.accuracy(input=model, label=label)\n",
    "\n",
    "# 执行器\n",
    "place = fluid.CUDAPlace(0)\n",
    "exe = fluid.Executor(place)\n",
    "exe.run(fluid.default_startup_program())\n",
    "\n",
    "# reader\n",
    "## 训练集reader\n",
    "tr_reader = train_reader(train_file_path)\n",
    "batch_train_reader = paddle.batch(tr_reader, batch_size=128)\n",
    "## 测试集reader\n",
    "ts_reader = test_reader(test_file_path)\n",
    "batch_test_reader = paddle.batch(ts_reader, batch_size=128)\n",
    "\n",
    "# feeder\n",
    "feeder = fluid.DataFeeder(place=place, feed_list=[words, label])\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(80):  # 外层循环控制训练轮次\n",
    "    for batch_id, data in enumerate(batch_train_reader()):  # 内层循环控制批次\n",
    "        train_cost, train_acc = exe.run(fluid.default_main_program(),  #program\n",
    "                                        feed=feeder.feed(data),  #喂入的参数\n",
    "                                        fetch_list=[avg_cost, accuracy])  #返回值\n",
    "        if batch_id % 100 == 0:\n",
    "            print(\"epoch:%d, batch:%d, cost:%f, acc:%f\" %\n",
    "                  (epoch, batch_id, train_cost[0], train_acc[0]))\n",
    "\n",
    "    # 每轮训练结束后进行模型评估\n",
    "    test_costs_list = []  # 存放测试集损失值\n",
    "    test_accs_list = []  # 存放测试集准确率\n",
    "\n",
    "    for batch_id, data in enumerate(batch_test_reader()):\n",
    "        test_cost, test_acc = exe.run(fluid.default_main_program(),\n",
    "                                      feed=feeder.feed(data),\n",
    "                                      fetch_list=[avg_cost, accuracy])\n",
    "        test_costs_list.append(test_cost[0])\n",
    "        test_accs_list.append(test_acc[0])\n",
    "    #　计算所有批次损失值/准确率均值\n",
    "    avg_test_cost = sum(test_costs_list) / len(test_costs_list)\n",
    "    avg_test_acc = sum(test_accs_list) / len(test_accs_list)\n",
    "    print(\"epoch:%d, test_cost:%f, test_acc:%f\" %\n",
    "          (epoch, avg_test_cost, avg_test_acc))\n",
    "\n",
    "# 训练结束，保存模型\n",
    "model_save_dir = \"model/\"\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)\n",
    "fluid.io.save_inference_model(model_save_dir,  # 保存路径\n",
    "                              feeded_var_names=[words.name],  # 使用时传入参数名称\n",
    "                              target_vars=[model],  #预测结果\n",
    "                              executor=exe)  #执行器\n",
    "print(\"模型保存成功.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 推理预测"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_save_dir = \"model/\"\n",
    "\n",
    "\n",
    "def get_data(sentence):  # 将传入的句子根据字典中的值进行编码\n",
    "    with open(dict_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        dict_txt = eval(f.readlines()[0])\n",
    "\n",
    "    ret = []  # 编码结果\n",
    "    keys = dict_txt.keys()\n",
    "    for w in sentence:  # 取出每个字\n",
    "        if not w in keys:  # 字不在字典中\n",
    "            w = \"<unk>\"\n",
    "        ret.append(int(dict_txt[w]))\n",
    "    return ret\n",
    "\n",
    "\n",
    "# 执行器\n",
    "place = fluid.CPUPlace()\n",
    "exe = fluid.Executor(place)\n",
    "exe.run(fluid.default_startup_program())\n",
    "\n",
    "infer_program, feed_names, target_var = fluid.io.load_inference_model(model_save_dir, exe)\n",
    "\n",
    "texts = []  # 存放待预测句子\n",
    "\n",
    "data1 = get_data(\"在获得诺贝尔文学奖7年之后，莫言15日晚间在山西汾阳贾家庄如是说\")\n",
    "data2 = get_data(\"综合'今日美国'、《世界日报》等当地媒体报道，芝加哥河滨警察局表示\")\n",
    "data3 = get_data(\"中国队2022年冬奥会表现优秀\")\n",
    "data4 = get_data(\"中国人民银行今日发布通知，降低准备金率，预计释放4000亿流动性\")\n",
    "data5 = get_data(\"10月20日,第六届世界互联网大会正式开幕\")\n",
    "data6 = get_data(\"同一户型，为什么高层比低层要贵那么多？\")\n",
    "data7 = get_data(\"揭秘A股周涨5%资金动向：追捧2类股，抛售600亿香饽饽\")\n",
    "data8 = get_data(\"宋慧乔陷入感染危机，前夫宋仲基不戴口罩露面，身处国外神态轻松\")\n",
    "data9 = get_data(\"此盆栽花很好养，花美似牡丹，三季开花，南北都能养，很值得栽培\")  # 不属于任何一个类别\n",
    "\n",
    "texts.append(data1)\n",
    "texts.append(data2)\n",
    "texts.append(data3)\n",
    "texts.append(data4)\n",
    "texts.append(data5)\n",
    "texts.append(data6)\n",
    "texts.append(data7)\n",
    "texts.append(data8)\n",
    "texts.append(data9)\n",
    "\n",
    "base_shape = [[len(c) for c in texts]]  # 计算每个句子长度\n",
    "tensor_words = fluid.create_lod_tensor(texts, base_shape, place)\n",
    "result = exe.run(infer_program,\n",
    "                 feed={feed_names[0]: tensor_words},\n",
    "                 fetch_list=target_var)\n",
    "names = [\"文化\", \"娱乐\", \"体育\", \"财经\", \"房产\", \"汽车\", \"教育\", \"科技\", \"国际\", \"证券\"]\n",
    "for r in result[0]:\n",
    "    idx = np.argmax(r)  # 取出最大值的索引\n",
    "    print(\"预测结果:\", names[idx], \" 概率:\", r[idx])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
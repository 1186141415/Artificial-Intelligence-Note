### day01

- 有监督学习：在训练数据中，有输出数据的为有监督学习
- 无监督学习：在训练数据中，没有输出数据的为无监督学习
    - 根据数据的相似程度，划分类别
    - 没有对错，只有好坏
- 回归问题：预测值为连续值
- 分类问题：预测值为离散值
- 聚类问题：聚类属于无监督学习，根据数据的相似程度划分为不同的群落
- fit_transform是两步操作
    - fit()
    - transform()



### day02

- 矩阵相乘：A的所有行 * B的所有列。对应位置相乘再相加

- A的列数  ==  B的行数，才能相乘

- 结果的维度：(A的行数,B的列数)

- 梯度下降：求损失函数的极小值

    - 按照负梯度的方式进行参数更新

    - 负梯度：比极小值小，往大走

        ​                比极小值大，往小走

    - 离极小值越远，下降的越快

    - 离极小值越近，下降的越慢

- 超参数：在构建模型之前，需要设定的参数。决定模型的精度

    - 超参数的取值，大部分取决于经验

- 在训练模型中fit(x,y)

    - x必须是二维数据
    - y最好是一维数据

- 回归问题的损失函数：均方误差



### day03

- 欠拟合：数据分布比较复杂，模型选择的比较简单。模型表达能力不足
- 过拟合：过于拟合训练数据，对训练数据非常好，但测试数据非常差
- 线性模型变种模型的本质：在损失函数后加上正则化，来防止过拟合
    - Lasso回归 ： 损失函数 + L1范数正则化
    - 岭回归: 损失函数 + L2范数正则化
- 在划分数据时，数据一定不能有顺序（随机）
- 单颗决策树，属于弱模型
- Adaboost不断的调整待预测样本数据的权重值



### day04

回归问题简单总结

- 线性回归
- 岭回归，Lasso回归
- 多项式回归
- 决策树回归
    - Adaboost
    - GBDT
    - 随机森林
- 损失函数：均方误差
- 评估指标：
    - 平均绝对误差
    - 中位数绝对偏差
    - r2得分



- 逻辑回归

    - 根据样本数据，构建线性回归模型，得到预测值（连续，线性）
    - 将连续的线性的预测值---》带入逻辑函数sigmoid
    - 将预测值映射到0-1区间内，将线性输出转为非线性
    - 设定阈值0.5,   大于0.5---》1   小于等于0.5--->0

- 查准率：对的个数 / 预测出来的个数

- 召回率：对的个数 / 真实的样本个数

- 查准率和召回率是分开类别单独统计的（每个类别都有自己的查准率和召回率）

- 分类报告:

- ```
    sm.classification_report(test_y,pred_test_y)
    ```

    

- 交叉验证：
    - 一般多用于：模型优化中，针对全部样本进行评估
    - 单独使用：模型构建之后，开始训练之前



- 随着决策树的划分，样本纯度是在不断的提升
- 纯度不断的提升，信息熵在不断的减小
- ID3-》信息增益来作为划分最优分割特征的标准
- C4.5->使用增益率来作为划分最优分割特征的标准



### day05

- 验证曲线：一次只能验证一个模型参数
- 一个好的数据，能够构建好的模型，一个不好的数据，一定不能够构建好的模型
- 分类业务：优先查看类别是否均衡
- 类别均衡：
    - 上采样
    - 下采样
    - 样本不够，权重来凑class_weight='balanced'



- 事件概率： P（A）    P（B）







### day06

- 文档频率和样本语义贡献度  呈反相关
    - 文档频率越高，对判断类别越没用
- 文档频率和逆文档频率呈反相关
- 逆文档频率和样本语义贡献度呈正相关
    - 逆文档频率越高，对判断类别越有用



- 分类模型简单总结
    - 逻辑回归
    - 决策树分类
        - 信息熵
        - 信息增益-》ID3
        - 增益率-》C4.5
        - Gini系数-》cart
    - 支持向量机SVM
        - 正确性、安全性、公平性、简单性
        - 软间隔、硬间隔
        - 解决线性不可分（升维变换）
        - 核函数：线性核函数、多项式核函数、径向基核函数
        - 适用于少量样本数据
    - 朴素贝叶斯
        - 朴素  +  贝叶斯
        - 朴素：特征之间相互独立
        - 贝叶斯：贝叶斯定理
        - GaussianNB、MultinomalNB、BernoulliNB
        - 场景：特征之间尽量相互独立、根据先验概率计算后验概率
    - 分类业务的损失函数(代价函数):
        - 交叉熵
    - 分类业务的评估指标
        - 精度：对的个数 / 总个数
        - 查准率：对的个数 / 预测出来的个数
        - 召回率：对的个数 / 真实的样本个数
        - f1得分: 2*查准率 *召回率 / 查准率 + 召回率
        - 混淆矩阵：
            - 主对角线的值是各个类别对的个数
            - 每一行的和为真实样本个数
            - 每一列的和为预测样本个数
        - 分类报告
    - 模型优化
        - 验证曲线：寻找最优模型参数（一次只能找一个）
        - 学习曲线：寻找最优训练集和测试集占比
        - 网格搜索：利用穷举法列出所有参数组合，寻找最优参数组合
        - 样本类别均衡化：
            - 上采样
            - 下采样
            - 类别均衡 class_weight='balanced'
        - 置信概率：属于每个类别的概率 predict_proba



### day07

1. 项目背景
2. 需求拆解
3. 收集数据
4. 数据清洗，数据预处理
5. 建立模型
6. 优化模型
7. 保存部署
8. 交付结果



### day08

- 感知机(神经元): 线性模型

- 异或：相同返回0，不同返回1（二进制相加不进位）

- 一个神经元得到一个输出

- tanh常用于NLP

- Relu常用于图像领域

- softmax用于分类模型中的最后一层（输出层）

    将输出结果转为相对概率



### day09

- 灰度图像：默认的色彩空间：GRAY
- 彩色图像：默认的色彩空间：BGR
    - HSV-》 H：色相，S：饱和度，V：亮度
    - YUV-》Y：亮度 , UV:色差信号
- 彩色图像的shape:
    - 三维的数组(高,宽,通道数)
- 灰度图像的shape:
    - 二维数组(高，宽)
- 彩色图像能够转为灰度图像，灰度图像不能转为彩色图像
- imread时， flags=1 彩色图像， flags=0灰度图像
- 二值化与反二值化：需要的内容变为白色，不想要的内容变为黑色





### day10

- 图像的加法：相加的图像尺寸必须一致
    - 对应位置像素值对应相加（导致图像偏白）
- 图像的开运算：先腐蚀，在膨胀
- 图像的闭运算：先膨胀，在腐蚀
- 形态学梯度：膨胀-腐蚀
- 垂直方向像素值，向下是正数，向上是负数
- 卷积神经网络，接收的图像，大小必须一致
- 对于训练集：随机裁剪
- 对于测试集：中心裁剪





### day11

- 轮廓坐标
    - 列表套三维数组
    - 三维数组：[[[1,2]],[[2,3]],[[4,5]],[[6,7]]]         （4,1,2）
- 轮廓的层级关系：
    - [Next,Previous,First_Child,Parent]  下一个，上一个，第一个子轮廓，父轮廓
    - 如果有对应的轮廓，返回对应的轮廓索引，如果没有返回-1



- 绘制轮廓时，线条粗细给-1,则为实心化填充
- 拟合多边形
    - 精度采用周长作为参考值，周长的多少倍
    - 精度值越大，精度越低
    - 精度值越小，精度越高





### day12

- 张量不能持久化存储，变量可以持久化存储(写入文件中)
- 模型参数，使用变量创建（变量可以持久化）
- 占位符一般用于样本数据中
    - 输入数据x 格式为:   (None,特征数)

- 运行占位符时，必须传入具体数据，否则报错
- 新增op节点，只能添加到默认的图上面
- 用到变量，必须进行初始化，否则报错

- 深度学习读取数据要求：快，随机，批量















